{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaForConditionalGeneration, LlavaConfig, CLIPVisionConfig, PhiConfig\n",
    "\n",
    "# Initializing a CLIP-vision config\n",
    "vision_config = CLIPVisionConfig()\n",
    "\n",
    "# Initializing a Llama config\n",
    "text_config = PhiConfig()\n",
    "\n",
    "# Initializing a Llava llava-1.5-7b style configuration\n",
    "configuration = LlavaConfig(vision_config, text_config)\n",
    "\n",
    "# Initializing a model from the llava-1.5-7b style configuration\n",
    "model = LlavaForConditionalGeneration(configuration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaConfig {\n",
       "  \"ignore_index\": -100,\n",
       "  \"image_token_index\": 32000,\n",
       "  \"model_type\": \"llava\",\n",
       "  \"projector_hidden_act\": \"gelu\",\n",
       "  \"text_config\": {\n",
       "    \"embd_pdrop\": 0.0,\n",
       "    \"hidden_act\": \"gelu_new\",\n",
       "    \"hidden_size\": 2048,\n",
       "    \"intermediate_size\": 8192,\n",
       "    \"layer_norm_eps\": 1e-05,\n",
       "    \"model_type\": \"phi\",\n",
       "    \"num_hidden_layers\": 24,\n",
       "    \"partial_rotary_factor\": 0.5,\n",
       "    \"qk_layernorm\": false,\n",
       "    \"resid_pdrop\": 0.0,\n",
       "    \"vocab_size\": 51200\n",
       "  },\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"vision_config\": {\n",
       "    \"hidden_size\": 768,\n",
       "    \"image_size\": 224,\n",
       "    \"intermediate_size\": 3072,\n",
       "    \"model_type\": \"clip_vision_model\",\n",
       "    \"num_attention_heads\": 12,\n",
       "    \"num_hidden_layers\": 12,\n",
       "    \"patch_size\": 32,\n",
       "    \"projection_dim\": 512\n",
       "  },\n",
       "  \"vision_feature_layer\": -2,\n",
       "  \"vision_feature_select_strategy\": \"default\"\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaForConditionalGeneration, LlavaConfig, CLIPVisionConfig, LlamaConfig\n",
    "\n",
    "# Initializing a CLIP-vision config\n",
    "vision_config2 = CLIPVisionConfig()\n",
    "\n",
    "# Initializing a Llama config\n",
    "text_config2 = LlamaConfig()\n",
    "\n",
    "# Initializing a Llava llava-1.5-7b style configuration\n",
    "configuration2 = LlavaConfig(vision_config2, text_config2)\n",
    "\n",
    "# Initializing a model from the llava-1.5-7b style configuration\n",
    "model2 = LlavaForConditionalGeneration(configuration2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.config.text_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiConfig {\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"embd_pdrop\": 0.0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu_new\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8192,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"phi\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"partial_rotary_factor\": 0.5,\n",
       "  \"qk_layernorm\": false,\n",
       "  \"resid_pdrop\": 0.0,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 51200\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.text_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not exist: embd_pdrop\n",
      "not equal: hidden_act\n",
      "not equal: hidden_size\n",
      "not equal: intermediate_size\n",
      "not exist: layer_norm_eps\n",
      "not equal: model_type\n",
      "not equal: num_hidden_layers\n",
      "not exist: partial_rotary_factor\n",
      "not exist: qk_layernorm\n",
      "not exist: resid_pdrop\n",
      "not equal: vocab_size\n"
     ]
    }
   ],
   "source": [
    "for key, value in temp.items():\n",
    "    if key in temp2.keys():\n",
    "        if temp2[key] != value:\n",
    "            print(f\"not equal: {key}\")\n",
    "    if key not in temp2.keys():\n",
    "        print(f\"not exist: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi:\n",
      "hidden_size: 2048\n",
      "vocab_size: 51200\n"
     ]
    }
   ],
   "source": [
    "print(\"phi:\")\n",
    "print(f\"hidden_size: {model.config.text_config.hidden_size}\")\n",
    "print(f\"vocab_size: {model.config.text_config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama:\n",
      "hidden_size: 4096\n",
      "vocab_size: 32000\n"
     ]
    }
   ],
   "source": [
    "print(\"llama:\")\n",
    "print(f\"hidden_size: {model2.config.text_config.hidden_size}\")\n",
    "print(f\"vocab_size: {model2.config.text_config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiConfig {\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"embd_pdrop\": 0.0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu_new\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8192,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"phi\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"partial_rotary_factor\": 0.5,\n",
       "  \"qk_layernorm\": false,\n",
       "  \"resid_pdrop\": 0.0,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 51200\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration.text_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPImageProcessor {\n",
       "  \"_valid_processor_keys\": [\n",
       "    \"images\",\n",
       "    \"do_resize\",\n",
       "    \"size\",\n",
       "    \"resample\",\n",
       "    \"do_center_crop\",\n",
       "    \"crop_size\",\n",
       "    \"do_rescale\",\n",
       "    \"rescale_factor\",\n",
       "    \"do_normalize\",\n",
       "    \"image_mean\",\n",
       "    \"image_std\",\n",
       "    \"do_convert_rgb\",\n",
       "    \"return_tensors\",\n",
       "    \"data_format\",\n",
       "    \"input_data_format\"\n",
       "  ],\n",
       "  \"crop_size\": {\n",
       "    \"height\": 336,\n",
       "    \"width\": 336\n",
       "  },\n",
       "  \"do_center_crop\": true,\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"processor_class\": \"LlavaProcessor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"shortest_edge\": 336\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='llava-hf/llava-1.5-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<image>'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.decode([32000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_added_token = \"kirimirikir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.add_tokens([\"kirimirikir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32002"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.convert_tokens_to_ids(\"kirimirikir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kirimirikir'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.decode(32002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32001"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [27, 15636, 29], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<pad>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 2, 2, 2, 2, 2, 321, 343, 318, 994], [945, 25786, 373, 994, 7415, 290, 1909, 290, 339, 318]], 'attention_mask': [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"amir is here\", \"arsalan was here yesterday and today and he is\"], padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration.text_config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.image_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.config.image_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' coercion']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([32000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaConfig {\n",
       "  \"ignore_index\": -100,\n",
       "  \"image_token_index\": 32000,\n",
       "  \"model_type\": \"llava\",\n",
       "  \"projector_hidden_act\": \"gelu\",\n",
       "  \"text_config\": {\n",
       "    \"embd_pdrop\": 0.0,\n",
       "    \"hidden_act\": \"gelu_new\",\n",
       "    \"hidden_size\": 2048,\n",
       "    \"intermediate_size\": 8192,\n",
       "    \"layer_norm_eps\": 1e-05,\n",
       "    \"model_type\": \"phi\",\n",
       "    \"num_hidden_layers\": 24,\n",
       "    \"partial_rotary_factor\": 0.5,\n",
       "    \"qk_layernorm\": false,\n",
       "    \"resid_pdrop\": 0.0,\n",
       "    \"vocab_size\": 51200\n",
       "  },\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"vision_config\": {\n",
       "    \"hidden_size\": 768,\n",
       "    \"image_size\": 224,\n",
       "    \"intermediate_size\": 3072,\n",
       "    \"model_type\": \"clip_vision_model\",\n",
       "    \"num_attention_heads\": 12,\n",
       "    \"num_hidden_layers\": 12,\n",
       "    \"patch_size\": 32,\n",
       "    \"projection_dim\": 512\n",
       "  },\n",
       "  \"vision_feature_layer\": -2,\n",
       "  \"vision_feature_select_strategy\": \"default\"\n",
       "}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /home/arsalan/.cache/huggingface/hub/models--openai--clip-vit-large-patch14/snapshots/32bd64288804d66eefd0ccbe215aa642df71cc41/preprocessor_config.json\n",
      "size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 224. Converted to {'shortest_edge': 224}.\n",
      "crop_size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "Image processor CLIPImageProcessor {\n",
      "  \"_valid_processor_keys\": [\n",
      "    \"images\",\n",
      "    \"do_resize\",\n",
      "    \"size\",\n",
      "    \"resample\",\n",
      "    \"do_center_crop\",\n",
      "    \"crop_size\",\n",
      "    \"do_rescale\",\n",
      "    \"rescale_factor\",\n",
      "    \"do_normalize\",\n",
      "    \"image_mean\",\n",
      "    \"image_std\",\n",
      "    \"do_convert_rgb\",\n",
      "    \"return_tensors\",\n",
      "    \"data_format\",\n",
      "    \"input_data_format\"\n",
      "  ],\n",
      "  \"crop_size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/arsalan/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/vocab.json\n",
      "loading file merges.txt from cache at /home/arsalan/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/merges.txt\n",
      "loading file tokenizer.json from cache at /home/arsalan/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/tokenizer.json\n",
      "loading file added_tokens.json from cache at /home/arsalan/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /home/arsalan/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/arsalan/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPImageProcessor, AutoTokenizer\n",
    "\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPImageProcessor {\n",
       "  \"_valid_processor_keys\": [\n",
       "    \"images\",\n",
       "    \"do_resize\",\n",
       "    \"size\",\n",
       "    \"resample\",\n",
       "    \"do_center_crop\",\n",
       "    \"crop_size\",\n",
       "    \"do_rescale\",\n",
       "    \"rescale_factor\",\n",
       "    \"do_normalize\",\n",
       "    \"image_mean\",\n",
       "    \"image_std\",\n",
       "    \"do_convert_rgb\",\n",
       "    \"return_tensors\",\n",
       "    \"data_format\",\n",
       "    \"input_data_format\"\n",
       "  ],\n",
       "  \"crop_size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  },\n",
       "  \"do_center_crop\": true,\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"shortest_edge\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token_id = max(vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50294"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[27, 9060, 29]], 'attention_mask': [[1, 1, 1]]}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"<image>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\n",
      "image\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([27]))\n",
    "print(tokenizer.decode([9060]))\n",
    "print(tokenizer.decode([29]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_token = \"<image>\"\n",
    "tokenizer.add_tokens([image_token])\n",
    "\n",
    "#pad_token = \"<pad>\"\n",
    "#tokenizer.add_tokens([pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [50295], 'attention_mask': [1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(image_token))\n",
    "#print(tokenizer(pad_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "#tokenizer.pad_token_id = 50296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[50256, 50256, 50256, 50256, 50256, 5661, 318, 1790], [5661, 318, 257, 2392, 2420, 351, 220, 50295]], 'attention_mask': [[0, 0, 0, 0, 0, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"this is short\", \"this is a longer text with <image>\"], padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaProcessor\n",
    "processor = LlavaProcessor(image_processor=image_processor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class ImageTextInstructionFollowingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, image_folder_path=None):\n",
    "        self.data = data\n",
    "        self.image_folder_path = image_folder_path\n",
    "\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        conversation = self.data[\"conversations\"][index]\n",
    "        image_path = self.data[\"image\"][index]\n",
    "        if self.image_folder_path is not None:\n",
    "            image_path = f\"{self.image_folder_path}/{image_path}\"\n",
    "        img = Image.open(image_path)\n",
    "        instruction = conversation[0][\"value\"]\n",
    "        answer = conversation[1][\"value\"]\n",
    "\n",
    "        instruction_answer = instruction + \"\\n\\nAnswer:\" + answer\n",
    "        return instruction_answer, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        instruction_answers = [item[0] for item in examples]\n",
    "        images = [item[1] for item in examples]\n",
    "        \n",
    "        input_ids = self.processor.tokenizer(\n",
    "            instruction_answers, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        pixel_values = self.processor.image_processor(\n",
    "            #images=images, return_tensors=\"pt\", padding=True\n",
    "            images=images, return_tensors=\"pt\"\n",
    "        )[\"pixel_values\"]\n",
    "\n",
    "        batch = {\n",
    "            'input_ids': input_ids,\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': input_ids\n",
    "        }\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "train_df = pd.read_json(\"/home/arsalan/Desktop/multimodal_LLM-master/multimodal_LLM-master/data/chat_train.json\", lines=True)\n",
    "dataset = ImageTextInstructionFollowingDataset(data=train_df, image_folder_path=\"/home/arsalan/Desktop/multimodal_LLM-master/multimodal_LLM-master/data/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_collator = MyCustomDataCollator(processor)\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsalan/anaconda3/envs/vlm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPVisionConfig, AutoConfig, LlavaConfig\n",
    "vision_config = CLIPVisionConfig.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "vision_config._name_or_path = \"openai/clip-vit-large-patch14\"\n",
    "text_config = AutoConfig.from_pretrained(\"microsoft/phi-1_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = LlavaConfig(vision_config, text_config)\n",
    "configuration._attn_implementation = \"flash_attention_2\"\n",
    "configuration.image_token_index = 50295\n",
    "configuration.pad_token_id = 50256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers import LlavaForConditionalGeneration, CLIPVisionModel,AutoModelForCausalLM\n",
    "\n",
    "class LlavaMultiModalProjector(nn.Module):\n",
    "    def __init__(self, config: LlavaConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(config.vision_config.hidden_size, config.text_config.hidden_size, bias=True)\n",
    "        self.act = ACT2FN[config.projector_hidden_act]\n",
    "        self.linear_2 = nn.Linear(config.text_config.hidden_size, config.text_config.hidden_size, bias=True)\n",
    "\n",
    "    def forward(self, image_features):\n",
    "        hidden_states = self.linear_1(image_features)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.linear_2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class CustomLlavaForConditionalGeneration(LlavaForConditionalGeneration):\n",
    "    def __init__(self, config: LlavaConfig):\n",
    "        super(LlavaForConditionalGeneration, self).__init__(config)  # Use super() to call the parent class constructor\n",
    "        self.vision_tower = CLIPVisionModel.from_pretrained(config.vision_config._name_or_path,\n",
    "                                                            torch_dtype=torch.float32)\n",
    "\n",
    "        self.multi_modal_projector = LlavaMultiModalProjector(config).to(torch.float32)\n",
    "        self.vocab_size = config.text_config.vocab_size\n",
    "        self.language_model = AutoModelForCausalLM.from_pretrained(config.text_config._name_or_path,\n",
    "                                                                   torch_dtype=torch.float32, \n",
    "                                                                   #attn_implementation=config._attn_implementation\n",
    "                                                                   )\n",
    "        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n",
    "        self.post_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in CustomLlavaForConditionalGeneration is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n"
     ]
    }
   ],
   "source": [
    "model = CustomLlavaForConditionalGeneration(configuration).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50295"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.image_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_module_devices(module):\n",
    "    devices = {param.device for param in module.parameters()}\n",
    "    return devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Modal Projector Devices: {device(type='cuda', index=0)}\n",
      "Vision Tower Device: {device(type='cuda', index=0)}\n",
      "Language Model Device: {device(type='cuda', index=0)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Multi-Modal Projector Devices:\", check_module_devices(model.multi_modal_projector))\n",
    "print(\"Vision Tower Device:\", check_module_devices(model.vision_tower))\n",
    "print(\"Language Model Device:\", check_module_devices(model.language_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!! param requries grad False for LLM and vision clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definign the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPImageProcessor, AutoTokenizer\n",
    "\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token_id = max(vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50294"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[27, 9060, 29]], 'attention_mask': [[1, 1, 1]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"<image>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_token = \"<image>\"\n",
    "tokenizer.add_tokens([image_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50295"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(image_token)['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [50295], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<image>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [50295], 'attention_mask': [1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(image_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[50256, 50256, 50256, 50256, 50256, 5661, 318, 1790], [5661, 318, 257, 2392, 2420, 351, 220, 50295]], 'attention_mask': [[0, 0, 0, 0, 0, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"this is short\", \"this is a longer text with <image>\"], padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaProcessor\n",
    "processor = LlavaProcessor(image_processor=image_processor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [50295], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer(\"<image>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Dataset and collator fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class ImageTextInstructionFollowingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, image_folder_path=None):\n",
    "        self.data = data\n",
    "        self.image_folder_path = image_folder_path\n",
    "\n",
    "        #self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        conversation = self.data[\"conversations\"][index]\n",
    "        image_path = self.data[\"image\"][index]\n",
    "        if self.image_folder_path is not None:\n",
    "            image_path = f\"{self.image_folder_path}/{image_path}\"\n",
    "        img = Image.open(image_path)\n",
    "        instruction = conversation[0][\"value\"]\n",
    "        answer = conversation[1][\"value\"]\n",
    "\n",
    "        instruction_answer = instruction + \"\\n\\nAnswer:\" + answer\n",
    "        return instruction_answer, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        instruction_answers = [item[0] for item in examples]\n",
    "        images = [item[1] for item in examples]\n",
    "        \n",
    "        tokenizer_outputs = self.processor.tokenizer(\n",
    "            instruction_answers, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokenizer_outputs['input_ids']\n",
    "        attention_mask = tokenizer_outputs['attention_mask']\n",
    "\n",
    "        pixel_values = self.processor.image_processor(\n",
    "            #images=images, return_tensors=\"pt\", padding=True\n",
    "            images=images, return_tensors=\"pt\"\n",
    "        )[\"pixel_values\"]\n",
    "\n",
    "        #if torch.isnan(input_ids).any() or torch.isinf(input_ids).any():\n",
    "        #    raise ValueError(\"NaN or Inf found in input_ids during batching\")\n",
    "\n",
    "        #if torch.isnan(attention_mask).any() or torch.isinf(attention_mask).any():\n",
    "        #    raise ValueError(\"NaN or Inf found in attention_mask during batching\")\n",
    "\n",
    "        #if torch.isnan(pixel_values).any() or torch.isinf(pixel_values).any():\n",
    "        #    raise ValueError(\"NaN or Inf found in pixel_values during batching\")\n",
    "\n",
    "        batch = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': input_ids\n",
    "        }\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [321, 343, 318, 994], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer(\"amir is here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "train_df = pd.read_json(\"/home/arsalan/Desktop/multimodal_LLM-master/multimodal_LLM-master/data/chat_train.json\", lines=True)\n",
    "train_dataset = ImageTextInstructionFollowingDataset(data=train_df, image_folder_path=\"/home/arsalan/Desktop/multimodal_LLM-master/multimodal_LLM-master/data/images\")\n",
    "\n",
    "eval_df = pd.read_json(\"/home/arsalan/Desktop/multimodal_LLM-master/multimodal_LLM-master/data/chat_val_5K.json\", lines=True)\n",
    "eval_dataset = ImageTextInstructionFollowingDataset(data=eval_df, image_folder_path=\"/home/arsalan/Desktop/multimodal_LLM-master/multimodal_LLM-master/data/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_collator = MyCustomDataCollator(processor)\n",
    "dataloader = DataLoader(eval_dataset, batch_size=2, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key,val in batch.items():\n",
    "#    batch[key] = batch[key].to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.forward(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsalan/anaconda3/envs/vlm/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    #report_to = 'wandb',\n",
    "    output_dir=\"./outputs/phi_adaptor-test3\",\n",
    "    remove_unused_columns=False,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-5,\n",
    "    gradient_accumulation_steps=16,\n",
    "    per_device_train_batch_size=4,\n",
    "    #bf16=True,\n",
    "    #bf16_full_eval=True,\n",
    "    warmup_steps=0,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    lr_scheduler_kwargs={},\n",
    "    max_steps=3000,\n",
    "    dataloader_num_workers=14,\n",
    "    logging_steps=10,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    label_names=[\"labels\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_tower.vision_model.embeddings.class_embedding: requires_grad = False\n",
      "vision_tower.vision_model.embeddings.patch_embedding.weight: requires_grad = False\n",
      "vision_tower.vision_model.embeddings.position_embedding.weight: requires_grad = False\n",
      "vision_tower.vision_model.pre_layrnorm.weight: requires_grad = False\n",
      "vision_tower.vision_model.pre_layrnorm.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm2.weight: requires_grad = False\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm2.bias: requires_grad = False\n",
      "vision_tower.vision_model.post_layernorm.weight: requires_grad = False\n",
      "vision_tower.vision_model.post_layernorm.bias: requires_grad = False\n",
      "multi_modal_projector.linear_1.weight: requires_grad = True\n",
      "multi_modal_projector.linear_1.bias: requires_grad = True\n",
      "multi_modal_projector.linear_2.weight: requires_grad = True\n",
      "multi_modal_projector.linear_2.bias: requires_grad = True\n",
      "language_model.model.embed_tokens.weight: requires_grad = False\n",
      "language_model.model.layers.0.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.0.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.0.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.0.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.0.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.0.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.0.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.0.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.0.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.0.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.0.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.0.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.0.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.0.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.1.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.1.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.1.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.1.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.1.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.1.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.1.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.1.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.1.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.1.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.1.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.1.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.1.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.1.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.2.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.2.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.2.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.2.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.2.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.2.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.2.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.2.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.2.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.2.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.2.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.2.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.2.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.2.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.3.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.3.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.3.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.3.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.3.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.3.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.3.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.3.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.3.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.3.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.3.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.3.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.3.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.3.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.4.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.4.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.4.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.4.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.4.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.4.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.4.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.4.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.4.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.4.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.4.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.4.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.4.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.4.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.5.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.5.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.5.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.5.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.5.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.5.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.5.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.5.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.5.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.5.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.5.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.5.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.5.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.5.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.6.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.6.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.6.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.6.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.6.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.6.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.6.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.6.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.6.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.6.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.6.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.6.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.6.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.6.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.7.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.7.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.7.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.7.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.7.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.7.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.7.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.7.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.7.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.7.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.7.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.7.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.7.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.7.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.8.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.8.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.8.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.8.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.8.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.8.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.8.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.8.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.8.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.8.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.8.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.8.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.8.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.8.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.9.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.9.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.9.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.9.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.9.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.9.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.9.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.9.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.9.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.9.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.9.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.9.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.9.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.9.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.10.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.10.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.10.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.10.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.10.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.10.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.10.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.10.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.10.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.10.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.10.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.10.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.10.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.10.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.11.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.11.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.11.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.11.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.11.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.11.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.11.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.11.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.11.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.11.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.11.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.11.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.11.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.11.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.12.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.12.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.12.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.12.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.12.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.12.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.12.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.12.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.12.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.12.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.12.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.12.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.12.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.12.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.13.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.13.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.13.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.13.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.13.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.13.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.13.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.13.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.13.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.13.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.13.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.13.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.13.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.13.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.14.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.14.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.14.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.14.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.14.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.14.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.14.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.14.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.14.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.14.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.14.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.14.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.14.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.14.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.15.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.15.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.15.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.15.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.15.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.15.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.15.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.15.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.15.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.15.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.15.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.15.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.15.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.15.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.16.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.16.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.16.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.16.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.16.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.16.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.16.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.16.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.16.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.16.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.16.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.16.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.16.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.16.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.17.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.17.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.17.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.17.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.17.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.17.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.17.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.17.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.17.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.17.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.17.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.17.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.17.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.17.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.18.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.18.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.18.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.18.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.18.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.18.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.18.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.18.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.18.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.18.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.18.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.18.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.18.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.18.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.19.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.19.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.19.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.19.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.19.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.19.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.19.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.19.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.19.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.19.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.19.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.19.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.19.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.19.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.20.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.20.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.20.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.20.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.20.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.20.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.20.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.20.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.20.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.20.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.20.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.20.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.20.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.20.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.21.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.21.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.21.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.21.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.21.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.21.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.21.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.21.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.21.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.21.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.21.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.21.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.21.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.21.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.22.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.22.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.22.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.22.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.22.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.22.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.22.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.22.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.22.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.22.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.22.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.22.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.22.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.22.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.layers.23.self_attn.q_proj.weight: requires_grad = False\n",
      "language_model.model.layers.23.self_attn.q_proj.bias: requires_grad = False\n",
      "language_model.model.layers.23.self_attn.k_proj.weight: requires_grad = False\n",
      "language_model.model.layers.23.self_attn.k_proj.bias: requires_grad = False\n",
      "language_model.model.layers.23.self_attn.v_proj.weight: requires_grad = False\n",
      "language_model.model.layers.23.self_attn.v_proj.bias: requires_grad = False\n",
      "language_model.model.layers.23.self_attn.dense.weight: requires_grad = False\n",
      "language_model.model.layers.23.self_attn.dense.bias: requires_grad = False\n",
      "language_model.model.layers.23.mlp.fc1.weight: requires_grad = False\n",
      "language_model.model.layers.23.mlp.fc1.bias: requires_grad = False\n",
      "language_model.model.layers.23.mlp.fc2.weight: requires_grad = False\n",
      "language_model.model.layers.23.mlp.fc2.bias: requires_grad = False\n",
      "language_model.model.layers.23.input_layernorm.weight: requires_grad = False\n",
      "language_model.model.layers.23.input_layernorm.bias: requires_grad = False\n",
      "language_model.model.final_layernorm.weight: requires_grad = False\n",
      "language_model.model.final_layernorm.bias: requires_grad = False\n",
      "language_model.lm_head.weight: requires_grad = False\n",
      "language_model.lm_head.bias: requires_grad = False\n"
     ]
    }
   ],
   "source": [
    "# Freeze language_model and vision_tower parameters\n",
    "for param in model.language_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.vision_tower.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Verify that only multi_modal_projector parameters require gradients\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad = {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for param in model.parameters():\n",
    "    # Check if parameter dtype is  Float (float32)\n",
    "#    if param.dtype == torch.bfloat16:\n",
    "#        param.data = param.data.to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 6295552\n"
     ]
    }
   ],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "trainable_params = count_trainable_parameters(model)\n",
    "print(f\"Number of trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "            model,\n",
    "            training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-12 15:01:11,444] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsalan/anaconda3/envs/vlm/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamirarsalan-rajabi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arsalan/Desktop/multimodal_LLM-master/multimodal_LLM-master/huggingface/wandb/run-20240712_150115-5yiv304w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amirarsalan-rajabi/huggingface/runs/5yiv304w' target=\"_blank\">./outputs/phi_adaptor-test3</a></strong> to <a href='https://wandb.ai/amirarsalan-rajabi/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amirarsalan-rajabi/huggingface' target=\"_blank\">https://wandb.ai/amirarsalan-rajabi/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amirarsalan-rajabi/huggingface/runs/5yiv304w' target=\"_blank\">https://wandb.ai/amirarsalan-rajabi/huggingface/runs/5yiv304w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5500e8df190f470dacf1dddfff092835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.0464, 'grad_norm': 6.137228965759277, 'learning_rate': 9.996666666666669e-06, 'epoch': 0.0}\n",
      "{'loss': 4.9284, 'grad_norm': 5.80035924911499, 'learning_rate': 9.993333333333333e-06, 'epoch': 0.0}\n",
      "{'loss': 4.9527, 'grad_norm': 5.846682071685791, 'learning_rate': 9.990000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 4.9416, 'grad_norm': 14.239758491516113, 'learning_rate': 9.986666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 4.9133, 'grad_norm': 4.910815238952637, 'learning_rate': 9.983333333333333e-06, 'epoch': 0.0}\n",
      "{'loss': 4.7086, 'grad_norm': 6.767263412475586, 'learning_rate': 9.980000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 4.8151, 'grad_norm': 6.37697696685791, 'learning_rate': 9.976666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 4.892, 'grad_norm': 4.32615852355957, 'learning_rate': 9.973333333333333e-06, 'epoch': 0.0}\n",
      "{'loss': 4.6973, 'grad_norm': 4.271944046020508, 'learning_rate': 9.970000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 4.8359, 'grad_norm': 6.476644515991211, 'learning_rate': 9.966666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 4.8552, 'grad_norm': 6.985056400299072, 'learning_rate': 9.963333333333333e-06, 'epoch': 0.0}\n",
      "{'loss': 4.7542, 'grad_norm': 5.75981330871582, 'learning_rate': 9.960000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 4.7858, 'grad_norm': 7.177633285522461, 'learning_rate': 9.956666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 4.7712, 'grad_norm': 3.87751841545105, 'learning_rate': 9.953333333333333e-06, 'epoch': 0.0}\n",
      "{'loss': 4.8195, 'grad_norm': 3.7148141860961914, 'learning_rate': 9.950000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 4.7387, 'grad_norm': 3.9552693367004395, 'learning_rate': 9.946666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 4.7311, 'grad_norm': 3.900768518447876, 'learning_rate': 9.943333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 4.6906, 'grad_norm': 5.4269866943359375, 'learning_rate': 9.940000000000001e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vlm/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vlm/lib/python3.10/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/vlm/lib/python3.10/site-packages/transformers/trainer.py:3250\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3248\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/anaconda3/envs/vlm/lib/python3.10/site-packages/accelerate/accelerator.py:2134\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2134\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vlm/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vlm/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vlm/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-100"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.ignore_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
